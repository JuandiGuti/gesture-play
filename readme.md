# Gesture-Play: Control Multimedia por Gestos con ASL

## Descripci√≥n del Proyecto

Este proyecto tiene como objetivo implementar un sistema de reconocimiento de gestos utilizando visi√≥n por computadora y aprendizaje autom√°tico. El sistema es capaz de identificar ciertas letras del alfabeto en lengua de se√±as americana (ASL) y utilizar estos gestos para controlar la reproducci√≥n de videos en una interfaz web.

## Tecnolog√≠as Utilizadas

* **Python 3.10** (requerido por MediaPipe)
* **Flask** para el backend
* **MediaPipe** para la detecci√≥n de manos y extracci√≥n de puntos
* **OpenCV** para procesamiento de im√°genes
* **Scikit-learn** para entrenamiento de modelos SVM
* **HTML/CSS/JS** para el frontend
* **YouTube IFrame API** para el reproductor

## Estructura del Proyecto

```
/
‚îú‚îÄ‚îÄ app.py                # Servidor Flask principal
‚îú‚îÄ‚îÄ model_train.py        # Entrenamiento del modelo SVM
‚îú‚îÄ‚îÄ model.py              # L√≥gica de predicci√≥n con MediaPipe
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ index.html        # Interfaz principal
‚îÇ   ‚îî‚îÄ‚îÄ config.html       # Panel de configuraci√≥n
‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îú‚îÄ‚îÄ style.css         # Estilos globales
‚îÇ   ‚îî‚îÄ‚îÄ src/main.js       # L√≥gica frontend
‚îú‚îÄ‚îÄ modelo_svm.pkl        # Modelo entrenado (se genera)
‚îú‚îÄ‚îÄ labels.pkl            # Etiquetas del modelo (se genera)
‚îî‚îÄ‚îÄ asl_alphabet_train/   # Dataset de entrenamiento (no incluido)
```

## Instrucciones de Ejecuci√≥n

1. Instalar Python 3.10
2. Crear un entorno virtual:

```bash
python3.10 -m venv venv
```

3. Activar el entorno virtual:

* Windows:

```bash
venv\Scripts\activate
```

* Mac/Linux:

```bash
source venv/bin/activate
```

4. Instalar dependencias:

```bash
pip install -r requirements.txt
```

5. Ejecutar el backend:

```bash
python app.py
```

6. Acceder desde el navegador a:

```
http://localhost:8000
```

## Dataset Utilizado

Este proyecto utiliza el dataset **ASL Alphabet**, que contiene im√°genes de manos representando cada letra del alfabeto en lenguaje de se√±as americano.

üîó Puedes descargar el dataset desde Kaggle:  
[ASL Alphabet Dataset ‚Äì Kaggle](https://www.kaggle.com/datasets/grassknoted/asl-alphabet)

Una vez descargado, col√≥calo dentro del directorio `backend/` con el nombre exacto:

backend/asl_alphabet_train/

java
Copiar
Editar

Aseg√∫rate de mantener la estructura original de carpetas (una por letra) para que el entrenamiento funcione correctamente.

## Entrenamiento del Modelo

Desde la pesta√±a "Configuraci√≥n" en la interfaz web:

* Presiona **"Entrenar Modelo"** para procesar el dataset y generar `modelo_svm.pkl` y `labels.pkl`
* El entrenamiento se ejecuta en segundo plano (gracias a hilos)

> Nota: se utiliza MediaPipe, por lo que se requiere buena iluminaci√≥n y gestos claros para una detecci√≥n confiable.

## Gestos Reconocidos

| Letra | Acci√≥n                  | Significado |
| ----- | ----------------------- | ----------- |
| **P** | Pausar / Reanudar video | Pause       |
| **U** | Subir volumen           | Up          |
| **D** | Bajar volumen           | Down        |
| **B** | Retroceder 10 segundos  | Before      |
| **A** | Avanzar 10 segundos     | After       |

## Notas T√©cnicas

* **Evita cerrar el CMD** inesperadamente: se usa `threading` para evitar que el servidor Flask se reinicie tras guardar archivos.
* **Modo solo reproductor:** disponible desde la p√°gina de configuraci√≥n (toggle en localStorage).
* **Expansible:** se puede entrenar con m√°s letras simplemente agregando nuevas carpetas al dataset y volviendo a entrenar.

---

Para cualquier duda t√©cnica, revisar los comentarios en los scripts `model.py` y `main.js`.
